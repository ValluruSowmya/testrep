{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNF068NYErDKz605Hkrtg0T",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ValluruSowmya/testrep/blob/main/spacex.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#cognitiveclass.ai logo\n",
        "#Space X Falcon 9 First Stage Landing Prediction\n",
        "#Assignment: Machine Learning Prediction\n",
        "#Estimated time needed: 60 minutes\n",
        "\n",
        "#Space X advertises Falcon 9 rocket launches on its website with a cost of 62 million dollars; other providers cost upward of 165 million dollars each, much of the savings is because Space X can reuse the first stage. Therefore if we can determine if the first stage will land, we can determine the cost of a launch. This information can be used if an alternate company wants to bid against space X for a rocket launch. In this lab, you will create a machine learning pipeline to predict if the first stage will land given the data from the preceding labs.\n",
        "\n",
        "\n",
        "\n",
        "#Several examples of an unsuccessful landing are shown here:\n",
        "\n",
        "\n",
        "\n",
        "#Most unsuccessful landings are planned. Space X performs a controlled landing in the oceans.#\n",
        "\n",
        "#Objectives\n",
        "#Perform exploratory Data Analysis and determine Training Labels\n",
        "#Create a column for the class\n",
        "#Standardize the data\n",
        "#Split into training data and test data\n",
        "#Find best Hyperparameter for SVM, Classification Trees and Logistic Regression\n",
        "#Find the method performs best using test data\n",
        "#Import Libraries and Define Auxiliary Functions\n",
        "# We will import the following libraries for the lab\n",
        "\n",
        "import itertools\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score, precision_score, recall_score, roc_auc_score\n",
        "This function is to plot the confusion matrix.\n",
        "\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "Load the dataframe\n",
        "Load the data\n",
        "\n",
        "data = pd.read_csv(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DS0321EN-SkillsNetwork/datasets/dataset_part_2.csv\")\n",
        "data.head()\n",
        "FlightNumber\tDate\tBoosterVersion\tPayloadMass\tOrbit\tLaunchSite\tOutcome\tFlights\tGridFins\tReused\tLegs\tLandingPad\tBlock\tReusedCount\tSerial\tLongitude\tLatitude\tClass\n",
        "0\t1\t2010-06-04\tFalcon 9\t6104.959412\tLEO\tCCAFS SLC 40\tNone None\t1\tFalse\tFalse\tFalse\tNaN\t1.0\t0\tB0003\t-80.577366\t28.561857\t0\n",
        "1\t2\t2012-05-22\tFalcon 9\t525.000000\tLEO\tCCAFS SLC 40\tNone None\t1\tFalse\tFalse\tFalse\tNaN\t1.0\t0\tB0005\t-80.577366\t28.561857\t0\n",
        "2\t3\t2013-03-01\tFalcon 9\t677.000000\tISS\tCCAFS SLC 40\tNone None\t1\tFalse\tFalse\tFalse\tNaN\t1.0\t0\tB0007\t-80.577366\t28.561857\t0\n",
        "3\t4\t2013-09-29\tFalcon 9\t500.000000\tPO\tVAFB SLC 4E\tFalse Ocean\t1\tFalse\tFalse\tFalse\tNaN\t1.0\t0\tB1003\t-120.610829\t34.632093\t0\n",
        "4\t5\t2013-12-03\tFalcon 9\t3170.000000\tGTO\tCCAFS SLC 40\tNone None\t1\tFalse\tFalse\tFalse\tNaN\t1.0\t0\tB1004\t-80.577366\t28.561857\t0\n",
        "X = pd.read_csv('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DS0321EN-SkillsNetwork/datasets/dataset_part_3.csv')\n",
        "X.head()\n",
        "FlightNumber\tPayloadMass\tFlights\tBlock\tReusedCount\tOrbit_ES-L1\tOrbit_GEO\tOrbit_GTO\tOrbit_HEO\tOrbit_ISS\t...\tSerial_B1058\tSerial_B1059\tSerial_B1060\tSerial_B1062\tGridFins_False\tGridFins_True\tReused_False\tReused_True\tLegs_False\tLegs_True\n",
        "0\t1.0\t6104.959412\t1.0\t1.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t...\t0.0\t0.0\t0.0\t0.0\t1.0\t0.0\t1.0\t0.0\t1.0\t0.0\n",
        "1\t2.0\t525.000000\t1.0\t1.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t...\t0.0\t0.0\t0.0\t0.0\t1.0\t0.0\t1.0\t0.0\t1.0\t0.0\n",
        "2\t3.0\t677.000000\t1.0\t1.0\t0.0\t0.0\t0.0\t0.0\t0.0\t1.0\t...\t0.0\t0.0\t0.0\t0.0\t1.0\t0.0\t1.0\t0.0\t1.0\t0.0\n",
        "3\t4.0\t500.000000\t1.0\t1.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t...\t0.0\t0.0\t0.0\t0.0\t1.0\t0.0\t1.0\t0.0\t1.0\t0.0\n",
        "4\t5.0\t3170.000000\t1.0\t1.0\t0.0\t0.0\t0.0\t1.0\t0.0\t0.0\t...\t0.0\t0.0\t0.0\t0.0\t1.0\t0.0\t1.0\t0.0\t1.0\t0.0\n",
        "5 rows Ã— 83 columns\n",
        "\n",
        "TASK 1\n",
        "Create the outcome variable from the column Class in data, then assign it to the variable Y, make sure the output is a Pandas series (only one bracket df['name of column']).\n",
        "\n",
        "Y = data['Class']\n",
        "type(Y)\n",
        "pandas.core.series.Series\n",
        "df=Y.value_counts()\n",
        "plt.figure(figsize=(8,8))\n",
        "ax=sns.barplot(x=df.index, y=df.values, palette='hls', alpha=0.9)\n",
        "sns.despine(top=True, right=True, left=False, bottom=False)\n",
        "for p in ax.patches:\n",
        "    ax.annotate('n = {:.0f}'.format(p.get_height()), (p.get_x()+0.4, p.get_height()),\n",
        "                ha='center', va='bottom', color='black', fontsize=14)\n",
        "ax.set_xticklabels(['Unsuccessful', 'Successful'], minor=False, fontsize=12)\n",
        "plt.yticks(fontsize=12)\n",
        "plt.xticks(fontsize=12)\n",
        "plt.title('Launch Outcome Success Counts', fontsize=24)\n",
        "plt.ylabel('Number of Launches',fontsize=18)\n",
        "plt.xlabel('Outcome',fontsize=18)\n",
        "plt.show()\n",
        "\n",
        "TASK 2\n",
        "Use the function train_test_split to split the data sets X and Y into training and test data. Set the parameter test_size to 0.2 and random_state to 2.\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=2)\n",
        "X_train.shape\n",
        "(72, 83)\n",
        "X_test.shape\n",
        "(18, 83)\n",
        "After splitting the data, there are 72 records in our training set and 18 in our test set.\n",
        "\n",
        "TASK 3\n",
        "Use the function using fit_transform() to standardize the training data so that we can learn the scaling parameters of our training set. Then, use these learned parameters to scale our test data.\n",
        "\n",
        "scaler = preprocessing.StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "TASK 4\n",
        "Create a logistic regression object then create a GridSearchCV object logreg_cv with cv = 10. Fit the object to find the best parameters from the dictionary parameters.\n",
        "\n",
        "parameters ={'C':[0.01,0.1,1],\n",
        "             'penalty':['l2'],\n",
        "             'solver':['lbfgs']}\n",
        "lr=LogisticRegression(random_state=1)\n",
        "logreg_cv = GridSearchCV(lr, parameters, cv=10, refit=True)\n",
        "logreg_cv.fit(X_train, Y_train)\n",
        "GridSearchCV(cv=10, estimator=LogisticRegression(random_state=1),\n",
        "             param_grid={'C': [0.01, 0.1, 1], 'penalty': ['l2'],\n",
        "                         'solver': ['lbfgs']})\n",
        "We output the GridSearchCV object for logistic regression. We display the best parameters using the data attribute best_params\\_ and the accuracy on the validation data using the data attribute best_score\\_.\n",
        "\n",
        "print(\"Tuned hyperparameters:\",logreg_cv.best_params_)\n",
        "print(\"Cross-validation accuracy:\",logreg_cv.best_score_)\n",
        "Tuned hyperparameters: {'C': 0.01, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
        "Cross-validation accuracy: 0.8464285714285713\n",
        "TASK 5\n",
        "Calculate the accuracy on the test data using the method score:\n",
        "\n",
        "glm_acc=logreg_cv.score(X_test, Y_test)\n",
        "print(\"Test set accuracy: {:.1%}\".format(glm_acc))\n",
        "glm_probs = logreg_cv.predict_proba(X_test)[:,1]\n",
        "glm_auc=roc_auc_score(Y_test, glm_probs)\n",
        "print(\"Test set AUC: {:.3}\".format(glm_auc))\n",
        "Test set accuracy: 83.3%\n",
        "Test set AUC: 0.889\n",
        "Lets look at the confusion matrix:\n",
        "\n",
        "# Compute confusion matrix\n",
        "glm_yhat = logreg_cv.predict(X_test)\n",
        "glm_f1 = f1_score(Y_test, glm_yhat)\n",
        "glm_prec = precision_score(Y_test, glm_yhat)\n",
        "glm_rec = recall_score(Y_test, glm_yhat)\n",
        "cnf_matrix = confusion_matrix(Y_test, glm_yhat, labels=[1,0])\n",
        "np.set_printoptions(precision=2)\n",
        "print(classification_report(Y_test, glm_yhat))\n",
        "\n",
        "# Plot non-normalized confusion matrix\n",
        "plt.figure(figsize=(8,6))\n",
        "plot_confusion_matrix(cnf_matrix, classes=['Landed', 'Did not land'], normalize= False, title='GLM Confusion matrix')\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           0       1.00      0.50      0.67         6\n",
        "           1       0.80      1.00      0.89        12\n",
        "\n",
        "    accuracy                           0.83        18\n",
        "   macro avg       0.90      0.75      0.78        18\n",
        "weighted avg       0.87      0.83      0.81        18\n",
        "\n",
        "Confusion matrix, without normalization\n",
        "[[12  0]\n",
        " [ 3  3]]\n",
        "\n",
        "Examining the confusion matrix, we see that logistic regression can distinguish between the different classes. We see that the major problem is false positives.\n",
        "\n",
        "TASK 6\n",
        "Create a support vector machine object then create a GridSearchCV object svm_cv with cv - 10. Fit the object to find the best parameters from the dictionary parameters.\n",
        "\n",
        "parameters = {'kernel':('linear', 'rbf','poly','rbf', 'sigmoid'),\n",
        "              'C': np.logspace(-3, 3, 5),\n",
        "              'gamma':np.logspace(-3, 3, 5)}\n",
        "svm = SVC(probability=True, random_state=1)\n",
        "svm_cv = GridSearchCV(svm, parameters, cv=10)\n",
        "svm_cv.fit(X_train, Y_train)\n",
        "GridSearchCV(cv=10, estimator=SVC(probability=True, random_state=1),\n",
        "             param_grid={'C': array([1.00e-03, 3.16e-02, 1.00e+00, 3.16e+01, 1.00e+03]),\n",
        "                         'gamma': array([1.00e-03, 3.16e-02, 1.00e+00, 3.16e+01, 1.00e+03]),\n",
        "                         'kernel': ('linear', 'rbf', 'poly', 'rbf', 'sigmoid')})\n",
        "print(\"Tuned hyperparameters:\",svm_cv.best_params_)\n",
        "print(\"Cross-validation accuracy:\",svm_cv.best_score_)\n",
        "Tuned hyperparameters: {'C': 1.0, 'gamma': 0.03162277660168379, 'kernel': 'sigmoid'}\n",
        "Cross-validation accuracy: 0.8482142857142856\n",
        "TASK 7\n",
        "Calculate the accuracy on the test data using the method score:\n",
        "\n",
        "svm_acc=svm_cv.score(X_test, Y_test)\n",
        "print(\"Test set accuracy: {:.1%}\".format(svm_acc))\n",
        "svm_probs = svm_cv.predict_proba(X_test)[:,1]\n",
        "svm_auc=roc_auc_score(Y_test, svm_probs)\n",
        "print(\"Test set AUC: {:.3}\".format(svm_auc))\n",
        "Test set accuracy: 83.3%\n",
        "Test set AUC: 0.958\n",
        "We can plot the confusion matrix\n",
        "\n",
        "# Compute confusion matrix\n",
        "svm_yhat = svm_cv.predict(X_test)\n",
        "svm_f1 = f1_score(Y_test, svm_yhat)\n",
        "svm_prec = precision_score(Y_test, svm_yhat)\n",
        "svm_rec = recall_score(Y_test, svm_yhat)\n",
        "cnf_matrix = confusion_matrix(Y_test, svm_yhat, labels=[1,0])\n",
        "np.set_printoptions(precision=2)\n",
        "print(classification_report(Y_test, svm_yhat))\n",
        "\n",
        "# Plot non-normalized confusion matrix\n",
        "plt.figure(figsize=(8,6))\n",
        "plot_confusion_matrix(cnf_matrix, classes=['Landed', 'Did not land'], normalize=False, title='SVM Confusion matrix')\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           0       1.00      0.50      0.67         6\n",
        "           1       0.80      1.00      0.89        12\n",
        "\n",
        "    accuracy                           0.83        18\n",
        "   macro avg       0.90      0.75      0.78        18\n",
        "weighted avg       0.87      0.83      0.81        18\n",
        "\n",
        "Confusion matrix, without normalization\n",
        "[[12  0]\n",
        " [ 3  3]]\n",
        "\n",
        "TASK 8\n",
        "Create a decision tree classifier object then create a GridSearchCV object tree_cv with cv = 10. Fit the object to find the best parameters from the dictionary parameters.\n",
        "\n",
        "parameters = {'criterion': ['gini', 'entropy'],\n",
        "     'splitter': ['best', 'random'],\n",
        "     'max_depth': [2*n for n in range(1,10)],\n",
        "     'max_features': ['auto', 'sqrt'],\n",
        "     'min_samples_leaf': [1, 2, 4],\n",
        "     'min_samples_split': [2, 5, 10]}\n",
        "\n",
        "tree = DecisionTreeClassifier(random_state=1)\n",
        "tree_cv = GridSearchCV(tree, parameters, cv=10)\n",
        "tree_cv.fit(X_train, Y_train)\n",
        "GridSearchCV(cv=10, estimator=DecisionTreeClassifier(random_state=1),\n",
        "             param_grid={'criterion': ['gini', 'entropy'],\n",
        "                         'max_depth': [2, 4, 6, 8, 10, 12, 14, 16, 18],\n",
        "                         'max_features': ['auto', 'sqrt'],\n",
        "                         'min_samples_leaf': [1, 2, 4],\n",
        "                         'min_samples_split': [2, 5, 10],\n",
        "                         'splitter': ['best', 'random']})\n",
        "print(\"Tuned hyperparameters:\",tree_cv.best_params_)\n",
        "print(\"Cross-validation Accuracy:\",tree_cv.best_score_)\n",
        "Tuned hyperparameters: {'criterion': 'gini', 'max_depth': 4, 'max_features': 'auto', 'min_samples_leaf': 1, 'min_samples_split': 10, 'splitter': 'random'}\n",
        "Cross-validation Accuracy: 0.8357142857142857\n",
        "TASK 9\n",
        "Calculate the accuracy of tree_cv on the test data using the method score:\n",
        "\n",
        "tree_acc=tree_cv.score(X_test, Y_test)\n",
        "print(\"Test set accuracy: {:.1%}\".format(tree_acc))\n",
        "tree_probs = tree_cv.predict_proba(X_test)[:,1]\n",
        "tree_auc=roc_auc_score(Y_test, tree_probs)\n",
        "print(\"Test set AUC: {:.3}\".format(tree_auc))\n",
        "Test set accuracy: 77.8%\n",
        "Test set AUC: 0.792\n",
        "We can plot the confusion matrix\n",
        "\n",
        "# Compute confusion matrix\n",
        "tree_yhat = tree_cv.predict(X_test)\n",
        "tree_f1 = f1_score(Y_test, tree_yhat)\n",
        "tree_prec = precision_score(Y_test, tree_yhat)\n",
        "tree_rec = recall_score(Y_test, tree_yhat)\n",
        "cnf_matrix = confusion_matrix(Y_test, tree_yhat, labels=[1,0])\n",
        "np.set_printoptions(precision=2)\n",
        "print(classification_report(Y_test, tree_yhat))\n",
        "\n",
        "# Plot non-normalized confusion matrix\n",
        "plt.figure(figsize=(8,6))\n",
        "plot_confusion_matrix(cnf_matrix, classes=['Landed', 'Did not land'], normalize=False, title='Decision Tree Confusion matrix')\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           0       0.62      0.83      0.71         6\n",
        "           1       0.90      0.75      0.82        12\n",
        "\n",
        "    accuracy                           0.78        18\n",
        "   macro avg       0.76      0.79      0.77        18\n",
        "weighted avg       0.81      0.78      0.78        18\n",
        "\n",
        "Confusion matrix, without normalization\n",
        "[[9 3]\n",
        " [1 5]]\n",
        "\n",
        "TASK 10\n",
        "Create a k nearest neighbors object then create a GridSearchCV object knn_cv with cv = 10. Fit the object to find the best parameters from the dictionary parameters.\n",
        "\n",
        "parameters = {'n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
        "              'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
        "              'p': [1,2]}\n",
        "\n",
        "KNN = KNeighborsClassifier()\n",
        "knn_cv = GridSearchCV(KNN, parameters, cv=10)\n",
        "knn_cv.fit(X_train, Y_train)\n",
        "GridSearchCV(cv=10, estimator=KNeighborsClassifier(),\n",
        "             param_grid={'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
        "                         'n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
        "                         'p': [1, 2]})\n",
        "print(\"Tuned hyperparameters:\",knn_cv.best_params_)\n",
        "print(\"Cross-validation accuracy:\",knn_cv.best_score_)\n",
        "Tuned hyperparameters: {'algorithm': 'auto', 'n_neighbors': 10, 'p': 1}\n",
        "Cross-validation accuracy: 0.8482142857142858\n",
        "TASK 11\n",
        "Calculate the accuracy of tree_cv on the test data using the method accuracy_score:\n",
        "\n",
        "knn_acc = knn_cv.score(X_test, Y_test)\n",
        "print(\"Test set accuracy: {:.1%}\".format(knn_acc))\n",
        "knn_probs = knn_cv.predict_proba(X_test)[:,1]\n",
        "knn_auc=roc_auc_score(Y_test, knn_probs)\n",
        "print(\"Test set AUC: {:.3}\".format(knn_auc))\n",
        "Test set accuracy: 83.3%\n",
        "Test set AUC: 0.847\n",
        "We can plot the confusion matrix\n",
        "\n",
        "# Compute confusion matrix\n",
        "knn_yhat = knn_cv.predict(X_test)\n",
        "knn_f1 = f1_score(Y_test, knn_yhat)\n",
        "knn_prec = precision_score(Y_test, knn_yhat)\n",
        "knn_rec = recall_score(Y_test, knn_yhat)\n",
        "cnf_matrix = confusion_matrix(Y_test, knn_yhat, labels=[1,0])\n",
        "np.set_printoptions(precision=2)\n",
        "print(classification_report(Y_test, knn_yhat))\n",
        "\n",
        "# Plot non-normalized confusion matrix\n",
        "plt.figure(figsize=(8,6))\n",
        "plot_confusion_matrix(cnf_matrix, classes=['Landed', 'Did not land'], normalize=False, title='KNN Confusion matrix')\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           0       1.00      0.50      0.67         6\n",
        "           1       0.80      1.00      0.89        12\n",
        "\n",
        "    accuracy                           0.83        18\n",
        "   macro avg       0.90      0.75      0.78        18\n",
        "weighted avg       0.87      0.83      0.81        18\n",
        "\n",
        "Confusion matrix, without normalization\n",
        "[[12  0]\n",
        " [ 3  3]]\n",
        "\n",
        "TASK 12\n",
        "Find the method performs best:\n",
        "\n",
        "data = {'AUC': [glm_auc, svm_auc, tree_auc, knn_auc], 'F1-Score': [glm_f1, svm_f1, tree_f1, knn_f1],\n",
        "        'Precision': [glm_prec, svm_prec, tree_prec, knn_prec], 'Recall': [glm_rec, svm_rec, tree_rec, knn_rec],\n",
        "        'Accuracy': [glm_acc, svm_acc, tree_acc, knn_acc]}\n",
        "res = pd.DataFrame(data, index=['Logistic Regression', 'SVM', 'Decision Tree', 'KNN']).sort_values(by=['AUC'], ascending=False)\n",
        "res.round(3)\n",
        "AUC\tF1-Score\tPrecision\tRecall\tAccuracy\n",
        "SVM\t0.958\t0.889\t0.8\t1.00\t0.833\n",
        "Logistic Regression\t0.889\t0.889\t0.8\t1.00\t0.833\n",
        "KNN\t0.847\t0.889\t0.8\t1.00\t0.833\n",
        "Decision Tree\t0.792\t0.818\t0.9\t0.75\t0.778\n",
        "plt.figure(figsize=(12,8))\n",
        "ax=sns.barplot(x=res.index, y='Accuracy', data=res, palette='Blues_d')\n",
        "sns.despine(top=True, right=True, left=False, bottom=False)\n",
        "plt.xlabel('Model', fontsize=20)\n",
        "plt.ylabel('Accuracy', fontsize=20)\n",
        "ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: '{:.0f}%'.format(x*100)))\n",
        "for p in ax.patches:\n",
        "    ax.annotate('{:.1f}%'.format(p.get_height()*100), (p.get_x()+0.4, p.get_height()),\n",
        "                ha='center', va='bottom',color= 'black')\n",
        "plt.title('Model Accuracy on the Test Set', fontsize=20)\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(12,8))\n",
        "ax=sns.barplot(x=res.index, y='AUC', data=res, palette='Blues_d')\n",
        "sns.despine(top=True, right=True, left=False, bottom=False)\n",
        "plt.xlabel('Model', fontsize=20)\n",
        "plt.ylabel('Area Under the Curve', fontsize=20)\n",
        "for p in ax.patches:\n",
        "    ax.annotate('{:.3f}'.format(p.get_height()), (p.get_x()+0.4, p.get_height()),\n",
        "                ha='center', va='bottom',color='black')\n",
        "plt.title('Test Set Area Under the Curve', fontsize=20)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "FBwVVrgqtw78"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}